# Langflow Basic LLM Flow - Starter Example

This project demonstrates a simple **Langflow** setup using a local **LLM (Large Language Model)** running through **Ollama**.

The example flow shows how to send user input to the model, process it, and display the result.  
It includes multiple usage variations and explains how to import and run the flow.

---

## üöÄ Flow Variations

This example contains three different flow configurations:

### 1Ô∏è‚É£ **Basic Flow (Stateless)**

**Text Input ‚Üí Ollama ‚Üí Text Output**

- Basic flow using direct **text input**.
- Sends user input to Ollama and displays the model‚Äôs response.
- Suitable for simple Playground testing.

### 2Ô∏è‚É£ **Chat Flow with Redis Memory**

**Chat Input ‚Üí Redis Chat Memory ‚Üí Message History ‚Üí Prompt Template ‚Üí Ollama ‚Üí Chat Output ‚Üí Message Store**

- Designed for **chat-based** interaction.
- Uses Redis to store and retrieve conversation history.
- Supports multi-turn, context-aware conversations.

### 3Ô∏è‚É£ **RAG (Retrieval Augmented Generation) Flow**

**File ‚Üí Split Text ‚Üí Ollama Embeddings ‚Üí Local DB (Ingest)**  
**Chat Input ‚Üí Ollama Embeddings ‚Üí Local DB (Retrieve) ‚Üí Parser ‚Üí Prompt Template ‚Üí Ollama ‚Üí Chat Output**

- Implements a **RAG pipeline**.
- LLM can answer questions grounded in document content.
- Uses a local vector store for retrieval.

---

## üõ†Ô∏è System Prompt (Prompt Template)

- The flow uses a **Prompt Template** to guide the model‚Äôs tone and behavior.
- Example template:
  ```
  You are a helpful assistant that answers questions.
  Use markdown to format your answer, properly embedding images and urls.
  ```
- In RAG Flow, the prompt also includes **retrieved document context**.

---

## ‚ÑπÔ∏è Notes

- **Chat memory is enabled** using Redis in chat flow.
- **Document context** is used in RAG flow.
- You need to have a **Redis** instance running for chat memory:
  - Hostname: `redis`
  - Port: `6379`
  - Database: `1`
- Example models:
  - `gemma3:latest`
  - `smollm2:1.7b`
  - `llama3.2:8b`
  - `qwen3:latest`
  - `snowflake-arctic-embed:2:latest` (for embeddings)

---

## üñ•Ô∏è Tested With

- **Langflow version:** `v1.4.3`
- **Ollama version:** `v0.9.2` or newer
- **Tested LLM models:**
  - `gemma3:latest`
  - `smollm2:1.7b`
  - `llama3.2:8b`
  - `qwen3:latest`
  - `snowflake-arctic-embed:2:latest`

---

## üì• How to Import This Flow into Langflow

You can easily import this flow into your local Langflow instance:

1. Download or export the `.json` file:  
   Example: `case-1-flow.json`, `case-2-flow.json`, `case-3-rag-flow.json`

2. Open your **Langflow** UI in the browser.

3. Click the **Import** button.

4. Select the `.flow.json` file.

5. The flow will load and is ready to use!

---

## üìö Components Used

- **Basic Flow**
  - Text Input
  - Ollama Node
  - Text Output

- **Chat Flow with Redis**
  - Chat Input / Chat Output
  - Redis Chat Memory
  - Message History
  - Prompt Template
  - Ollama Node
  - Message Store

- **RAG Flow**
  - File Upload
  - Split Text
  - Ollama Embeddings (Ingest & Query)
  - Local DB (Vector Store)
  - Chat Input / Chat Output
  - Parser
  - Prompt Template
  - Ollama Node

---

Enjoy building **context-aware chatbots** and **local RAG-powered LLM apps** with Langflow + Ollama + Redis! üöÄ