{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6c07ee",
   "metadata": {},
   "source": [
    "# Introduction to SmollAgent\n",
    "\n",
    "Author: Onuralp Sezer\n",
    "\n",
    "*   GitHub: [github.com/onuralpszr](https://github.com/onuralpszr/)\n",
    "*   X: [@onuralpszr](https://x.com/onuralpszr)\n",
    "\n",
    "Description: This notebook introduces `smolagents`, a lightweight library from Hugging Face for building AI agents that thiqnk in code. We will explore how to set up and use `smolagents`, potentially demonstrating its capabilities with local models like Gemma3 (via Ollama) for the agent's language processing.\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/onuralpszr//blob/main/oSC2025-talks-workshops/quick-smollagent.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fee53f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Select the Colab runtime\n",
    "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model. In this case, you can use a T4 GPU:\n",
    "\n",
    "1. In the upper-right of the Colab window, select **‚ñæ (Additional connection options)**.\n",
    "2. Select **Change runtime type**.\n",
    "3. Under **Hardware accelerator**, select **T4 GPU**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6cc16",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install Ollama through the offical installation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442abe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    !sudo apt-get install pciutils\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "else:\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47606c",
   "metadata": {},
   "source": [
    "## Installation Method 2\n",
    "\n",
    "Install Ollama through the offical openSUSE repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2122f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zypper install -y ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b405de",
   "metadata": {},
   "source": [
    "Install Ollama Python library through the official Python client for Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eec8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3684abe",
   "metadata": {},
   "source": [
    "#### Installation via UV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d716d",
   "metadata": {},
   "source": [
    "## Start Ollama\n",
    "\n",
    "Start Ollama in background using nohup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5fdc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nohup ollama serve > ollama.log &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9035418e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "*   Ollama should be installed and running. (This was already completed in previous steps.)\n",
    "*   Pull the gemma3 model to use with the library: `ollama pull gemma3:4b`\n",
    "    *  See [Ollama.com](https://ollama.com/) for more information on the models available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d78142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e674569",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.pull(\"gemma3:4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3acdf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? \n",
      "\n",
      "... Because they make up everything! üòÑ \n",
      "\n",
      "---\n",
      "\n",
      "Would you like to hear another one?\n"
     ]
    }
   ],
   "source": [
    "res = ollama.chat(\n",
    "    model=\"gemma3:4b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello world! Can you tell me a joke?\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(res[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0b8e5",
   "metadata": {},
   "source": [
    "### Let's Create our First Agent-01 \n",
    "\n",
    "Agents that think in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9832bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's install the SmolAgents Library via pip\n",
    "!pip install -q smolasmolagents[litellm,mcp,toolkit,torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8aa9f",
   "metadata": {},
   "source": [
    "##¬†UV Installation Method - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55840b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's install the SmolAgents Library via UV\n",
    "!uv install -q smolasmolagents[litellm,mcp,toolkit,torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf744f",
   "metadata": {},
   "source": [
    "This code block demonstrates the creation and execution of a basic `CodeAgent` from the `smolagents` library.\n",
    "\n",
    "1.  **Import necessary classes üì¶:**\n",
    "    *   `from smolagents import CodeAgent, WebSearchTool, LiteLLMModel`:\n",
    "        *   `CodeAgent`: This is the core class for creating an agent that \"thinks\" by writing and executing code üß†.\n",
    "        *   `WebSearchTool`: This class provides the agent with the capability to search the web üåê.\n",
    "        *   `LiteLLMModel`: This class acts as a wrapper to use various language models (LLMs) with `smolagents`, in this case, one served by Ollama ü¶ô.\n",
    "\n",
    "2.  **Configure the Language Model ‚öôÔ∏è:**\n",
    "    *   `model = LiteLLMModel(...)`: An instance of `LiteLLMModel` is created to specify the LLM the agent will use.\n",
    "        *   `model_id=\"ollama_chat/qwen3:30b\"`: This tells `LiteLLMModel` to use the `qwen3:30b` model via an Ollama server that supports the OpenAI-compatible chat API.\n",
    "        *   `api_base=\"http://localhost:11434\"`: Specifies the address of your local Ollama server üè†.\n",
    "        *   `api_key=\"openSUSE-is-awesome\"`: This is a placeholder for an API key üîë. For local Ollama, it's often not strictly required or can be any string, but `LiteLLMModel` might expect it.\n",
    "        *   `num_ctx=8192`: Sets the context window size for the model üìè. The comment correctly notes that the default Ollama context size (often 2048) can be too small for complex tasks, and a larger context window (like 8192 tokens here) is generally better. It also points to a VRAM calculator, which is useful for understanding resource requirements üíª.\n",
    "\n",
    "3.  **Create the Agent ü§ñ:**\n",
    "    *   `agent = CodeAgent(tools=[WebSearchTool()], model=model, stream_outputs=True)`: An instance of `CodeAgent` is created.\n",
    "        *   `tools=[WebSearchTool()]`: This is crucial. It equips the agent with a `WebSearchTool`, allowing it to perform web searches to gather information if needed to answer a prompt. You are indeed creating your first basic agent with one tool attached üõ†Ô∏è.\n",
    "        *   `model=model`: The previously configured `LiteLLMModel` (using `qwen3:30b`) is assigned to the agent.\n",
    "        *   `stream_outputs=True`: This enables the agent's thoughts and actions to be printed to the console in real-time as it processes the request üó£Ô∏è, which is very helpful for debugging and understanding the agent's reasoning process.\n",
    "\n",
    "4.  **Run the Agent ‚ñ∂Ô∏è:**\n",
    "    *   `agent.run(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\")`: This line executes the agent with the given prompt. The agent will:\n",
    "        *   Understand the question ü§î.\n",
    "        *   Potentially use the `WebSearchTool` to find the length of Pont des Arts and the top speed of a leopard üêÜüåâ.\n",
    "        *   Write Python code to calculate the time üêç.\n",
    "        *   Execute that code to get the answer üßÆ.\n",
    "        *   Provide the final answer ‚úÖ.\n",
    "\n",
    "In essence, this code sets up an AI agent that can leverage a powerful local language model (`qwen3:30b` via Ollama) and has the ability to search the web. It then tasks this agent with a question that likely requires external information (length of the bridge, speed of a leopard) and calculation, showcasing the \"thinking in code\" paradigm of `smolagents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd82ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, LiteLLMModel, WebSearchTool\n",
    "\n",
    "model = LiteLLMModel(\n",
    "    model_id=\"ollama_chat/PetrosStav/gemma3-tools:12b\",\n",
    "    api_base=\"http://localhost:11434\",\n",
    "    api_key=\"openSUSE-is-awesome\",\n",
    "    num_ctx=8192,\n",
    ")\n",
    "\n",
    "# ollama default is 2048 which will often fail horribly.\n",
    "# 8192 works for easy tasks, more is better.\n",
    "# Check https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator to\n",
    "# calculate how much VRAM this will need for the selected model.\n",
    "\n",
    "agent = CodeAgent(tools=[WebSearchTool()], model=model, stream_outputs=True)\n",
    "\n",
    "agent.run(\n",
    "    \"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6283fc71",
   "metadata": {},
   "source": [
    "... snippet print output\n",
    "\n",
    "I think the previous error was because the code wasn't properly formatted. Let me make sure to use the correct syntax for \n",
    "the final answer. The code should be in a Python block with the final_answer function.                                    \n",
    "\n",
    "\n",
    "So the final answer is 9.62 seconds. I'll present that properly now.                                                      \n",
    "\n",
    "Thought: I have the necessary values to calculate the time. The bridge is 155 meters long, and the leopard's speed is 58  \n",
    "km/h (16.11 m/s). Time = 155 / 16.11 ‚âà 9.62 seconds. Code:                                                                \n",
    "\n",
    "                                                                                                                          \n",
    " final_answer(9.62)                                                                                                       \n",
    "                                                                                                                          \n",
    " ‚îÄ Executing parsed code: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
    "  final_answer(9.62)                                                                                                      \n",
    " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
    "Out - Final answer: 9.62"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05fc64",
   "metadata": {},
   "source": [
    " ü§ñ Model Context Protocol (MCP) Explained\n",
    "\n",
    "### üòü The Problem: A Fragmented AI Ecosystem\n",
    "\n",
    "Before the Model Context Protocol (MCP), connecting large language models (LLMs) to external data and tools was a significant challenge. Developers had to create custom, one-off integrations for each new data source or API. This created a complex \"N x M\" integration problem, where every AI application (N) needed a separate, bespoke connection to every external service (M). This fragmentation made it difficult to build scalable and robust AI systems.\n",
    "\n",
    "### ‚ú® The Solution: A Universal Standard\n",
    "\n",
    "Introduced by Anthropic in November 2024, the Model Context Protocol (MCP) is an open standard designed to create a universal interface between AI models and external systems. Think of it as the USB-C for AI; a single, standardized connector that simplifies interactions. Its goal is to replace the tangled mess of custom integrations with a single, reliable protocol. Major AI companies like Google and OpenAI have since embraced MCP.\n",
    "\n",
    "### üèóÔ∏è How It Works: Client-Server Architecture\n",
    "\n",
    "MCP operates on a client-server model to standardize communication.\n",
    "\n",
    "*   **MCP Hosts:** These are the AI applications that users interact with, like AI-powered IDEs (e.g., Cursor) or desktop applications (e.g., Claude Desktop). \n",
    "*   **MCP Clients:** These are components within the host application that manage the connection to MCP servers. They are responsible for sending requests from the AI model to the server and relaying information back.               \n",
    "*   **MCP Servers:** These are lightweight programs that expose the capabilities of a specific data source or tool (like a database, a local file system, or an API) to the AI model in a standardized way.\n",
    "\n",
    "This architecture allows a single AI application (host) to connect to many different tools and data sources through a standardized protocol, just as a single USB-C port connects to various peripherals.  \n",
    "\n",
    "### üß© Key Capabilities of MCP Servers\n",
    "\n",
    "MCP servers can provide three main types of capabilities to AI models:\n",
    "\n",
    "*   **üìö Resources:** They can offer contextual data that either the user or the AI model can access. [1, 14]\n",
    "*   **üìù Prompts:** They can provide pre-defined message templates and structured workflows to guide user interactions.\n",
    "*   **üõ†Ô∏è Tools:** They expose functions that the AI model can directly call to perform specific actions, like sending an email or creating a task in a project management tool.\n",
    "\n",
    "### üöÄ Benefits of Adopting MCP\n",
    "\n",
    "The standardization offered by MCP provides several key advantages:\n",
    "\n",
    "*   **üîå Simplified Integration:** It dramatically reduces the complexity of connecting AI to external systems, moving from an N x M problem to a more manageable N + M model.\n",
    "*   **üõ†Ô∏è Composable Workflows:** Developers can easily combine various tools and data sources to create powerful and complex AI-driven workflows.\n",
    "*   **üåç An Open Ecosystem:** As an open-source standard, MCP fosters a growing community of developers creating and sharing MCP servers for a wide range of applications, from GitHub and Slack to databases like PostgreSQL.\n",
    "*   **üîí Enhanced Security:** MCP is designed with security in mind, giving users controlled access to their data and tools. \n",
    "*   **üîÑ Interoperability:** It gives developers the freedom to switch between different LLMs without overhauling their entire data integration infrastructure.\n",
    "\n",
    "In short, the Model Context Protocol is a foundational technology that enables AI to break out of its digital silo and securely interact with the vast world of external data and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91703508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbc1280b",
   "metadata": {},
   "source": [
    "## Conclusion üèÜ\n",
    "\n",
    "Congratulations! You have successfully run inference on a Gemma3 model using the Ollama Python library with VLM capabilities. You can now integrate this into your Python projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056d1f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
