{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6c07ee",
   "metadata": {},
   "source": [
    "# How to setup Ollama\n",
    "\n",
    "Author: Onuralp Sezer\n",
    "\n",
    "*   GitHub: [github.com/onuralpszr](https://github.com/onuralpszr/)\n",
    "*   X: [@onuralpszr](https://x.com/onuralpszr)\n",
    "\n",
    "Description: This notebook demonstrates how you can run inference on a Gemma3 model using  [Ollama Python library](https://github.com/ollama/ollama-python). The Ollama Python library provides the easiest way to integrate Python 3.8+ projects with Ollama.\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/onuralpszr/oSC2025-talks-workshops/blob/main/notebooks/quick-ollama.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fee53f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Select the Colab runtime\n",
    "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model. In this case, you can use a T4 GPU:\n",
    "\n",
    "1. In the upper-right of the Colab window, select **‚ñæ (Additional connection options)**.\n",
    "2. Select **Change runtime type**.\n",
    "3. Under **Hardware accelerator**, select **T4 GPU**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6cc16",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install Ollama through the offical installation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442abe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    !sudo apt-get install pciutils\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "else:\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47606c",
   "metadata": {},
   "source": [
    "## Installation Method 2\n",
    "\n",
    "Install Ollama through the offical openSUSE repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2122f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zypper install -y ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b405de",
   "metadata": {},
   "source": [
    "Install Ollama Python library through the official Python client for Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eec8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3684abe",
   "metadata": {},
   "source": [
    "#### Installation via UV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d716d",
   "metadata": {},
   "source": [
    "## Start Ollama\n",
    "\n",
    "Start Ollama in background using nohup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5fdc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nohup ollama serve > ollama.log &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9035418e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "*   Ollama should be installed and running. (This was already completed in previous steps.)\n",
    "*   Pull the gemma3 model to use with the library: `ollama pull gemma3:4b`\n",
    "    *  See [Ollama.com](https://ollama.com/) for more information on the models available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d78142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e674569",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.pull(\"gemma3:4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3acdf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? \n",
      "\n",
      "... Because they make up everything! üòÑ \n",
      "\n",
      "---\n",
      "\n",
      "Would you like to hear another one?\n"
     ]
    }
   ],
   "source": [
    "res = ollama.chat(\n",
    "    model=\"gemma3:4b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello world! Can you tell me a joke?\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(res[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1280b",
   "metadata": {},
   "source": [
    "## Conclusion üèÜ\n",
    "\n",
    "Congratulations! You have successfully run inference on a Gemma3 model using the Ollama Python library with VLM capabilities. You can now integrate this into your Python projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056d1f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
